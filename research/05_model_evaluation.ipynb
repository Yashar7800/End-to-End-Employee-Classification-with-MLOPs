{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1fc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548cddb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Yashar\\\\End-to-End-Employee-Classification-with-MLOPs\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37aefca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be23500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Yashar\\\\End-to-End-Employee-Classification-with-MLOPs'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df4e88",
   "metadata": {},
   "source": [
    "- In this stage we need URI for MLFlow. \n",
    "- we need dagshub as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0107e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = 'https://dagshub.com/Yashar7800/End-to-End-Employee-Classification-with-MLOPs.mlflow'\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'yashar7800'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '@Lucy1378'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211d303",
   "metadata": {},
   "source": [
    "## config_entity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85ffade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    preprocessor_path: Path\n",
    "    model_path: Path\n",
    "    all_params: dict\n",
    "    metric_file_name: Path\n",
    "    target_column: str\n",
    "    mlflow_uri: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca667c58",
   "metadata": {},
   "source": [
    "## configuration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48135bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining Confuiguration\n",
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml,create_directories,save_json\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH,\n",
    "                 schema_filepath = SCHEMA_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.GradientBoostingClassifier\n",
    "        schema = self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            data_path = config.data_path,\n",
    "            preprocessor_path= config.preprocessor_path,\n",
    "            model_path = config.model_path,\n",
    "            all_params = params,\n",
    "            metric_file_name = config.metric_file_name,\n",
    "            target_column = schema.name,\n",
    "            mlflow_uri = \"https://dagshub.com/Yashar7800/End-to-End-Employee-Classification-with-MLOPs.mlflow\"\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7b85d6",
   "metadata": {},
   "source": [
    "## components/model_evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfccf37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, average_precision_score\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def eval_metrics(self, actual, pred, proba):\n",
    "        f1 = f1_score(actual, pred)\n",
    "        precision = precision_score(actual, pred)\n",
    "        recall = recall_score(actual, pred)\n",
    "        roc_auc = roc_auc_score(actual, proba)\n",
    "        avg_precision = average_precision_score(actual, proba)\n",
    "\n",
    "        return f1, precision, recall, roc_auc, avg_precision\n",
    "\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "        data['Tenure'] = 2025 - data['JoiningYear']\n",
    "        X = data.drop(self.config.target_column,axis =1)\n",
    "        y = data[self.config.target_column]\n",
    "\n",
    "        model = joblib.load(self.config.model_path) # loading the model\n",
    "\n",
    "        # Define stratified k-fold\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        # Initialize metrics\n",
    "        f1_scores, precisions, recalls, auc_roc_scores, auc_pr_scores = [], [], [], [], []\n",
    "\n",
    "\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri) # doing everything inside a remote server\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            # Cross-validation\n",
    "            for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                \n",
    "                # Fit model on training fold\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Predict on test fold\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                # Compute metrics\n",
    "                f1, precision, recall, roc_auc, avg_precision = self.eval_metrics(y_test, y_pred, y_proba)\n",
    "                \n",
    "                # Store metrics\n",
    "                f1_scores.append(f1)\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "                auc_roc_scores.append(roc_auc)\n",
    "                auc_pr_scores.append(avg_precision)\n",
    "                \n",
    "                # Log per-fold metrics\n",
    "                mlflow.log_metric(f\"f1_score_fold_{fold}\", f1)\n",
    "                mlflow.log_metric(f\"precision_fold_{fold}\", precision)\n",
    "                mlflow.log_metric(f\"recall_fold_{fold}\", recall)\n",
    "                mlflow.log_metric(f\"auc_roc_fold_{fold}\", roc_auc)\n",
    "                mlflow.log_metric(f\"auc_pr_fold_{fold}\", avg_precision)\n",
    "                print(f\"Fold {fold}: F1={f1:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, AUC-ROC={roc_auc:.4f}, AUC-PR={avg_precision:.4f}\")\n",
    "\n",
    "            # Compute average and std\n",
    "            avg_f1 = np.mean(f1_scores)\n",
    "            std_f1 = np.std(f1_scores)\n",
    "            avg_precision = np.mean(precisions)\n",
    "            avg_recall = np.mean(recalls)\n",
    "            avg_auc_roc = np.mean(auc_roc_scores)\n",
    "            avg_auc_pr = np.mean(auc_pr_scores)\n",
    "\n",
    "            # Log average metrics\n",
    "            mlflow.log_metric(\"avg_f1_score\", avg_f1)\n",
    "            mlflow.log_metric(\"std_f1_score\", std_f1)\n",
    "            mlflow.log_metric(\"avg_precision\", avg_precision)\n",
    "            mlflow.log_metric(\"avg_recall\", avg_recall)\n",
    "            mlflow.log_metric(\"avg_auc_roc\", avg_auc_roc)\n",
    "            mlflow.log_metric(\"avg_auc_pr\", avg_auc_pr)\n",
    "\n",
    "            # Save metrics locally using save_json\n",
    "            scores = {\n",
    "                \"avg_f1_score\": avg_f1,\n",
    "                \"std_f1_score\": std_f1,\n",
    "                \"avg_precision\": avg_precision,\n",
    "                \"avg_recall\": avg_recall,\n",
    "                \"avg_auc_roc\": avg_auc_roc,\n",
    "                \"avg_auc_pr\": avg_auc_pr,\n",
    "                \"f1_scores\": f1_scores,\n",
    "                \"precisions\": precisions,\n",
    "                \"recalls\": recalls,\n",
    "                \"auc_roc_scores\": auc_roc_scores,\n",
    "                \"auc_pr_scores\": auc_pr_scores\n",
    "            }\n",
    "            save_json(path=Path(self.config.metric_file_name), data=scores)\n",
    "\n",
    "            # Log model parameters\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "\n",
    "            # Log model\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"GradientBoostingClassifier\")\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "            print(f\"Average F1-score: {avg_f1:.4f}, Std: {std_f1:.4f}\")\n",
    "            print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "            print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "            print(f\"Average AUC-ROC: {avg_auc_roc:.4f}\")\n",
    "            print(f\"Average AUC-PR: {avg_auc_pr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9687db34",
   "metadata": {},
   "source": [
    "## pipeline/stage_05_model_evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a810fe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-15 14:16:01,797: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-07-15 14:16:01,812: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-07-15 14:16:01,815: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-07-15 14:16:01,817: INFO: common: created directory at: artifacts]\n",
      "[2025-07-15 14:16:01,818: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "Fold 1: F1=0.7816, Precision=0.8609, Recall=0.7156, AUC-ROC=0.8757, AUC-PR=0.8502\n",
      "Fold 2: F1=0.7660, Precision=0.8852, Recall=0.6750, AUC-ROC=0.8688, AUC-PR=0.8483\n",
      "Fold 3: F1=0.7555, Precision=0.8205, Recall=0.7000, AUC-ROC=0.8645, AUC-PR=0.8517\n",
      "Fold 4: F1=0.7879, Precision=0.8540, Recall=0.7312, AUC-ROC=0.8763, AUC-PR=0.8590\n",
      "Fold 5: F1=0.7560, Precision=0.8397, Recall=0.6875, AUC-ROC=0.8680, AUC-PR=0.8461\n",
      "[2025-07-15 14:16:24,070: INFO: common: json file saved at: artifacts\\model_evaluation\\metrics.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/15 14:16:29 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\Yashar\\AppData\\Local\\Temp\\tmp5s5zamf8\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.4.2', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/07/15 14:16:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'GradientBoostingClassifier' already exists. Creating a new version of this model...\n",
      "2025/07/15 14:16:35 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: GradientBoostingClassifier, version 2\n",
      "Created version '2' of model 'GradientBoostingClassifier'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1-score: 0.7694, Std: 0.0132\n",
      "Average Precision: 0.8521\n",
      "Average Recall: 0.7019\n",
      "Average AUC-ROC: 0.8706\n",
      "Average AUC-PR: 0.8511\n",
      "🏃 View run welcoming-steed-591 at: https://dagshub.com/Yashar7800/End-to-End-Employee-Classification-with-MLOPs.mlflow/#/experiments/0/runs/89cfe7fb95f24789b142e83a396f0822\n",
      "🧪 View experiment at: https://dagshub.com/Yashar7800/End-to-End-Employee-Classification-with-MLOPs.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation_config.log_into_mlflow()\n",
    "except Exception as e:\n",
    "    \n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
